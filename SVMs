#Answer to the question 1 first part
import numpy as np
import matplotlib.pyplot as plt
import oracle
from oracle import q1_fish_train_test_data

data1 = q1_fish_train_test_data(23647)

x_total = data1[1]
y_total = data1[2]

#l2 norms of means
n_values=[50, 100, 500, 1000, 2000, 4000]

def mu_calc(y_class,n,x=x_total):
    y_match_x = []
    for i,y in enumerate(y_total):
        if y==y_class and len(y_match_x)<=n:
            y_match_x.append(x[i])
    
    x_required = y_match_x[:n]
    
    mu_values = np.mean(x_required)
    mu_norm = np.linalg.norm(mu_values)
    return mu_norm, y_match_x, mu_values,len(y_match_x)

mu_s0 = []
for i in n_values:
    mu_s0.append(mu_calc(0,i)[0])

print(mu_s0)

plt.plot(n_values , mu_s0, color='g')    


mu_s1 = []
for i in n_values:
    mu_s1.append(mu_calc(1,i)[0])

print(mu_s1)

plt.plot(n_values , mu_s1, color='r')    
mu_s2 = []
for i in n_values:
    mu_s2.append(mu_calc(2,i)[0])

print(mu_s2)

plt.plot(n_values , mu_s2, color='y')    

mu_s3 = []
for i in n_values:
    mu_s3.append(mu_calc(3,i)[0])

print(mu_s3)

plt.plot(n_values , mu_s3, color='b')    

plt.xlabel('n_values')
plt.title('Trend of mu_norms with n')

plt.show()

#Frobenius norms of covariance matrices

def cov_norm(y_class,n,x=x_total):
    y_match_x = []
    for i,y in enumerate(y_total):
        if y==y_class & len(y_match_x)<=n:
            y_match_x.append(x[i])
    
    x_required = np.array(y_match_x[:n])
    x_required = np.array(x_required.reshape(n,-1).T)
    
    cov_matrix = np.cov(x_required)
    frob_norm = np.sqrt(np.sum(cov_matrix**2))
    return frob_norm,cov_matrix

fbn0 = []
for i in n_values:
    fbn0.append(cov_norm(0,i,x_total)[0])

print(fbn0)

plt.plot(n_values , fbn0, color='g') 

fbn1 = []
for i in n_values:
    fbn1.append(cov_norm(1,i,x_total)[0])

print(fbn1)

plt.plot(n_values , fbn1, color='r') 

fbn2 = []
for i in n_values:
    fbn2.append(cov_norm(2,i,x_total)[0])

print(fbn2)

plt.plot(n_values , fbn2, color='y') 

fbn3 = []
for i in n_values:
    fbn3.append(cov_norm(3,i,x_total)[0])

print(fbn3)

plt.plot(n_values , fbn3, color='b') 

plt.title('Trend of Frob_norm of cov matrices with n')
plt.show()

#Answer to first question second part 
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import oracle
from oracle import q1_fish_train_test_data

data1 = q1_fish_train_test_data(23647)
x_total = np.reshape(np.array(data1[1]), (len(data1[1]), -1)) 
y_total = np.array(data1[2])
x_test = np.reshape(np.array(data1[3]), (len(data1[3]), -1))
y_test = np.array(data1[4])


def generate_data(samples, x_data, y_data):
    indices = np.random.choice(len(x_data), samples, replace=False)
    return x_data[indices], y_data[indices]


def compute_means_and_covariance(X, y, n_classes):
    class_means = np.array([np.mean(X[y == c], axis=0) for c in range(n_classes)])
    pooled_cov = sum(np.cov(X[y == c].T) for c in range(n_classes)) / n_classes
    return class_means, pooled_cov

def compute_weight_vectors(class_means, pooled_cov):
    inv_cov = np.linalg.inv(pooled_cov)
    weight_vectors = np.array([inv_cov @ (class_means[c] - class_means[0]) for c in range(1, len(class_means))])
    return weight_vectors


def compute_best_weight_vector(class_means, pooled_cov):
    Sw = sum((class_means[c] - class_means[0]).reshape(-1, 1) @ (class_means[c] - class_means[0]).reshape(1, -1) 
             for c in range(1, len(class_means)))
    eigvals, eigvecs = np.linalg.eig(np.linalg.inv(pooled_cov) @ Sw)
    best_weight = eigvecs[:, np.argmax(eigvals)] 
    return best_weight

def compute_fld_objective(class_means, pooled_cov):
    Sw = sum((class_means[c] - class_means[0]).reshape(-1, 1) @ 
             (class_means[c] - class_means[0]).reshape(1, -1) 
             for c in range(1, len(class_means)))
    
    inv_Sw = np.linalg.inv(pooled_cov)
    J_value = np.trace(inv_Sw @ Sw) 
    return J_value

def compute_discriminant_scores(X, weight_vectors):
    scores = np.array([X @ weight_vectors[c] for c in range(len(weight_vectors))])
    return scores.T  

def project_data(X, weight_vectors):
    return X @ weight_vectors.T

def compute_accuracy(weight_vectors, x_test, y_test):
    scores = compute_discriminant_scores(x_test, weight_vectors)
    predicted_labels = np.argmax(scores, axis=1)  
    return np.mean(predicted_labels == y_test[:len(predicted_labels)])


n_val = [2500, 3500, 4000, 4500, 5000]
n_classes = 4

boxplot_data = {n: [] for n in n_values}
accuracies = {}

for n in n_val:
    acc_scores = []
    for _ in range(20): 
        X, y = generate_data(n, x_total, y_total)
        class_means, pooled_cov = compute_means_and_covariance(X, y, n_classes)
        weight_vectors = compute_weight_vectors(class_means, pooled_cov)
        best_weight = compute_best_weight_vector(class_means, pooled_cov)
        projections = project_data(X, weight_vectors)
        
        
        for c in range(n_classes - 1):  
            class_samples = projections[y == c]
            if len(class_samples) > 0: 
                boxplot_data[n].append(np.mean(class_samples))

        acc_scores.append(compute_accuracy(weight_vectors, x_test, y_test))
    
    accuracies[n] = np.mean(acc_scores)


plt.figure(figsize=(10, 6))
plt.boxplot([boxplot_data[n] for n in n_values], labels=n_values)
plt.title('Box Plot of Multi-class Objective Value for Different Sample Sizes')
plt.xlabel('Sample Size')
plt.ylabel('Objective Value (Projection Mean per Class)')
plt.show()


for n in n_values:
    X, y = generate_data(n, x_total, y_total)
    class_means, pooled_cov = compute_means_and_covariance(X, y, n_classes)
    weight_vectors = compute_weight_vectors(class_means, pooled_cov)
    projections = project_data(X, weight_vectors)
    
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    
    for c in range(n_classes - 1):  # Fix: Iterate only over existing projections
        ax.scatter(projections[y == c, 0], projections[y == c, 1], projections[y == c, 2], label=f'Class {c}', alpha=0.5)
    
    ax.set_title(f'3D Projection of Points onto FLD (n={n})')
    ax.set_xlabel('Projection Axis 1')
    ax.set_ylabel('Projection Axis 2')
    ax.set_zlabel('Projection Axis 3')
    ax.legend()
    plt.show()

# Print accuracy results
for n, acc in accuracies.items():
    print(f'Accuracy with {n} training samples: {acc:.4f}')

total_accuracy = compute_accuracy(weight_vectors, class_means, x_test, y_test)
print("Accuracy of my trained model:",total_accuracy)







